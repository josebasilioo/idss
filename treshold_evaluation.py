#!/usr/bin/env python
# coding: utf-8

"""
AVALIA√á√ÉO DE THRESHOLDS - Multi-Stage HIDS
Encontra os melhores thresholds tau_b, tau_m, tau_u atrav√©s de grid search
Gera tabela com melhores configura√ß√µes e justificativas
"""

import os
import numpy as np
import pandas as pd
import pickle
from sklearn.metrics import f1_score, balanced_accuracy_score, precision_score, recall_score, accuracy_score
from datetime import datetime
import time

# Configurar seeds
seed_value = 42
os.environ['PYTHONHASHSEED'] = str(seed_value)
import random
random.seed(seed_value)
np.random.seed(seed_value)

# Obter diret√≥rio do script
script_dir = os.path.dirname(os.path.abspath(__file__))
data_dir = os.path.join(script_dir, "data")
models_dir = os.path.join(script_dir, "modelos")
reports_dir = os.path.join(script_dir, "reports")

if not os.path.exists(reports_dir):
    os.makedirs(reports_dir)

print("üîç AVALIA√á√ÉO DE THRESHOLDS - Multi-Stage HIDS")
print("=" * 60)

# Carregar dados
print("üìä Carregando dados...")
test = pd.read_parquet(os.path.join(data_dir, "test.parquet"))
y_true = test["Y"].replace(["Heartbleed", "Infiltration"], "Unknown")
x = test.drop(columns=['Y'])

print(f"‚úÖ Dados carregados: {x.shape[0]} amostras, {x.shape[1]} features")
print("Distribui√ß√£o real:")
for classe, count in y_true.value_counts().items():
    print(f"  {classe}: {count:,} ({count/len(y_true)*100:.1f}%)")

# Carregar modelos
print("\nüîß Carregando modelos e escaladores...")
with open(os.path.join(models_dir, "stage1_ocsvm.p"), "rb") as f:
    stage1 = pickle.load(f)
with open(os.path.join(models_dir, "stage2_rf.p"), "rb") as f:
    stage2 = pickle.load(f)
with open(os.path.join(models_dir, "stage1_ocsvm_scaler.p"), "rb") as f:
    scaler_stage1 = pickle.load(f)
with open(os.path.join(models_dir, "stage2_rf_scaler.p"), "rb") as f:
    scaler_stage2 = pickle.load(f)

print("‚úÖ Modelos carregados")

def evaluate_pipeline(x_stage1, x_stage2, y_true, tau_b, tau_m, tau_u):
    """Avalia pipeline com thresholds espec√≠ficos e retorna m√©tricas detalhadas"""
    try:
        # Stage 1: Binary Detection
        score_test = -stage1.decision_function(x_stage1)
        y_pred = np.where(score_test < tau_b, "Benign", "Attack").astype(object)
        
        # Stage 2: Multi-class Classification
        if np.sum(y_pred == "Attack") > 0:
            x_attack_scaled = x_stage2[y_pred == "Attack"]
            y_proba_test_2 = stage2.predict_proba(x_attack_scaled)
            
            y_pred_2 = np.where(
                np.max(y_proba_test_2, axis=1) > tau_m, 
                stage2.classes_[np.argmax(y_proba_test_2, axis=1)], 
                'Unknown'
            )
            y_pred[y_pred == "Attack"] = y_pred_2
        
        # Stage 3: Zero-day Detection
        if np.sum(y_pred == "Unknown") > 0:
            unknown_scores = score_test[y_pred == "Unknown"]
            y_pred_3 = np.where(unknown_scores < tau_u, "Benign", "Unknown")
            y_pred[y_pred == "Unknown"] = y_pred_3
        
        # Calcular m√©tricas
        accuracy = accuracy_score(y_true, y_pred)
        balanced_acc = balanced_accuracy_score(y_true, y_pred)
        f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
        f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)
        recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)
        
        # M√©tricas espec√≠ficas
        benign_recall = recall_score(y_true == 'Benign', y_pred == 'Benign', zero_division=0)
        attack_detection_rate = np.sum((y_true != 'Benign') & (y_pred != 'Benign')) / np.sum(y_true != 'Benign')
        
        # Contagem de predi√ß√µes
        pred_counts = pd.Series(y_pred).value_counts().to_dict()
        
        return {
            'tau_b': tau_b, 'tau_m': tau_m, 'tau_u': tau_u,
            'accuracy': accuracy, 'balanced_accuracy': balanced_acc,
            'f1_macro': f1_macro, 'f1_weighted': f1_weighted,
            'precision_macro': precision_macro, 'recall_macro': recall_macro,
            'benign_recall': benign_recall, 'attack_detection_rate': attack_detection_rate,
            'pred_benign': pred_counts.get('Benign', 0),
            'pred_ddos': pred_counts.get('(D)DOS', 0),
            'pred_botnet': pred_counts.get('Botnet', 0),
            'pred_brute_force': pred_counts.get('Brute Force', 0),
            'pred_port_scan': pred_counts.get('Port Scan', 0),
            'pred_web_attack': pred_counts.get('Web Attack', 0),
            'pred_unknown': pred_counts.get('Unknown', 0),
            'total_attacks_predicted': sum([pred_counts.get(cls, 0) for cls in ['(D)DOS', 'Botnet', 'Brute Force', 'Port Scan', 'Web Attack']])
        }
    except Exception as e:
        return None

# OTIMIZA√á√ÉO 3: Usar apenas subset dos dados para busca inicial
print("\n‚öôÔ∏è Preparando dados...")
x_stage1_scaled = scaler_stage1.transform(x)
x_stage2_scaled = scaler_stage2.transform(x)

# Usar apenas 20% dos dados para busca r√°pida de thresholds
print("üî¨ Usando subset de 20% dos dados para otimiza√ß√£o r√°pida...")
subset_size = int(0.2 * len(x))
subset_indices = np.random.choice(len(x), subset_size, replace=False)

x_stage1_subset = x_stage1_scaled[subset_indices]
x_stage2_subset = x_stage2_scaled[subset_indices]
y_true_subset = y_true.iloc[subset_indices]

print(f"üìä Subset: {len(x_stage1_subset):,} amostras ({len(x_stage1_subset)/len(x)*100:.1f}% do total)")
print("Distribui√ß√£o do subset:")
for classe, count in y_true_subset.value_counts().items():
    print(f"  {classe}: {count:,} ({count/len(y_true_subset)*100:.1f}%)")

# Analisar scores para definir ranges inteligentes
print("üîç Analisando distribui√ß√£o de scores...")
scores_stage1 = -stage1.decision_function(x_stage1_scaled)
print(f"Stage 1 scores:")
print(f"  Range: [{np.min(scores_stage1):.6f}, {np.max(scores_stage1):.6f}]")
print(f"  Mean ¬± Std: {np.mean(scores_stage1):.6f} ¬± {np.std(scores_stage1):.6f}")
print(f"  Percentis: P5={np.percentile(scores_stage1, 5):.6f}, P50={np.percentile(scores_stage1, 50):.6f}, P95={np.percentile(scores_stage1, 95):.6f}")

# OTIMIZA√á√ÉO 1: Ranges muito menores e mais inteligentes
print("üß† Usando busca otimizada com ranges reduzidos...")

# Usar apenas valores cr√≠ticos baseados em percentis
tau_b_range = [
    np.percentile(scores_stage1, 10),  # Mais conservador
    np.percentile(scores_stage1, 25),  # Balanceado
    np.percentile(scores_stage1, 50)   # Mais agressivo
]

# Apenas valores de confian√ßa mais comuns
tau_m_range = [0.5, 0.7, 0.8, 0.9, 0.95]

# Apenas 3 valores para tau_u
tau_u_range = [
    np.percentile(scores_stage1, 20),
    np.percentile(scores_stage1, 40),
    np.percentile(scores_stage1, 60)
]

total_combinations = len(tau_b_range) * len(tau_m_range) * len(tau_u_range)

print(f"\nüìä Configura√ß√£o Grid Search:")
print(f"  ‚Ä¢ tau_b: {len(tau_b_range)} valores ({tau_b_range[0]:.6f} a {tau_b_range[-1]:.6f})")
print(f"  ‚Ä¢ tau_m: {len(tau_m_range)} valores ({min(tau_m_range):.2f} a {max(tau_m_range):.2f})")
print(f"  ‚Ä¢ tau_u: {len(tau_u_range)} valores ({tau_u_range[0]:.6f} a {tau_u_range[-1]:.6f})")
print(f"  ‚Ä¢ Total: {total_combinations:,} combina√ß√µes")

# OTIMIZA√á√ÉO 2: Busca sequencial inteligente
print(f"\nüöÄ Executando busca otimizada sequencial...")
print(f"üìä Total de {total_combinations} combina√ß√µes (3√ó5√ó3 = 45 ao inv√©s de 1200!)")
start_time = time.time()
results = []
current = 0

# Busca mais r√°pida com early stopping se necess√°rio
best_f1_so_far = 0
no_improvement_count = 0

for tau_b in tau_b_range:
    print(f"  üîç Testando tau_b = {tau_b:.6f}...")
    for tau_m in tau_m_range:
        for tau_u in tau_u_range:
            current += 1
            
            result = evaluate_pipeline(x_stage1_subset, x_stage2_subset, y_true_subset, tau_b, tau_m, tau_u)
            if result is not None:
                results.append(result)
                
                # Mostrar progresso a cada 10 itera√ß√µes
                if current % 10 == 0:
                    elapsed = time.time() - start_time
                    eta = (elapsed / current) * (total_combinations - current)
                    print(f"    Progresso: {current}/{total_combinations} ({current/total_combinations*100:.1f}%) - ETA: {eta:.1f}s")
                
                # Tracking do melhor resultado
                if result['f1_weighted'] > best_f1_so_far:
                    best_f1_so_far = result['f1_weighted']
                    no_improvement_count = 0
                    print(f"    ‚ú® Novo melhor F1: {best_f1_so_far:.4f}")
                else:
                    no_improvement_count += 1

elapsed_time = time.time() - start_time
print(f"‚úÖ Busca inicial conclu√≠da em {elapsed_time:.1f} segundos!")
print(f"üìä {len(results):,} combina√ß√µes testadas no subset")

# OTIMIZA√á√ÉO 4: Validar apenas os TOP 5 candidatos com dados completos
print(f"\nüéØ Validando TOP 5 candidatos com dados completos...")
results_df_subset = pd.DataFrame(results)
top_5_indices = results_df_subset.nlargest(5, 'f1_weighted').index

final_results = []
for i, idx in enumerate(top_5_indices):
    config = results_df_subset.iloc[idx]
    tau_b, tau_m, tau_u = config['tau_b'], config['tau_m'], config['tau_u']
    
    print(f"  {i+1}/5: Validando tau_b={tau_b:.6f}, tau_m={tau_m:.2f}, tau_u={tau_u:.6f}")
    
    # Avaliar com dados completos
    full_result = evaluate_pipeline(x_stage1_scaled, x_stage2_scaled, y_true, tau_b, tau_m, tau_u)
    if full_result is not None:
        final_results.append(full_result)

print(f"‚úÖ Valida√ß√£o completa finalizada!")
print(f"üìä {len(final_results)} configura√ß√µes validadas com dados completos")

# Usar resultados finais para an√°lise
results = final_results

# Analisar resultados
results_df = pd.DataFrame(results)

print(f"\nüèÜ AN√ÅLISE DOS MELHORES THRESHOLDS")
print("=" * 80)

# Definir m√©tricas e suas justificativas
metrics_analysis = [
    ('f1_weighted', 'F1-Score Weighted', 'Ideal para datasets desbalanceados como este (95% Benign)'),
    ('f1_macro', 'F1-Score Macro', 'Trata todas as classes igualmente, bom para an√°lise geral'),
    ('balanced_accuracy', 'Balanced Accuracy', 'Equilibra recall de todas as classes'),
    ('accuracy', 'Accuracy', 'M√©trica simples, mas pode ser enganosa em dados desbalanceados'),
    ('attack_detection_rate', 'Taxa Detec√ß√£o Ataques', 'Foco espec√≠fico na capacidade de detectar ataques'),
    ('benign_recall', 'Recall Benign', 'Importante para evitar falsos positivos excessivos')
]

# Criar tabela de melhores configura√ß√µes
best_configs = []
for metric_col, metric_name, justification in metrics_analysis:
    best = results_df.loc[results_df[metric_col].idxmax()]
    best_configs.append({
        'M√©trica': metric_name,
        'Valor': f"{best[metric_col]:.4f}",
        'tau_b': f"{best['tau_b']:.6f}",
        'tau_m': f"{best['tau_m']:.2f}",
        'tau_u': f"{best['tau_u']:.6f}",
        'Benign': f"{best['pred_benign']:,}",
        'Ataques': f"{best['total_attacks_predicted']:,}",
        'Unknown': f"{best['pred_unknown']:,}",
        'Justificativa': justification
    })

# Exibir tabela principal
main_table = pd.DataFrame(best_configs)
print("MELHORES CONFIGURA√á√ïES POR M√âTRICA:")
print("-" * 80)
display_cols = ['M√©trica', 'Valor', 'tau_b', 'tau_m', 'tau_u', 'Ataques', 'Unknown']
print(main_table[display_cols].to_string(index=False))

# Configura√ß√£o recomendada
recommended = results_df.loc[results_df['f1_weighted'].idxmax()]
print(f"\nüéØ CONFIGURA√á√ÉO RECOMENDADA (F1-Weighted - melhor para dados desbalanceados):")
print("=" * 60)
print(f"tau_b = {recommended['tau_b']:.6f}")
print(f"tau_m = {recommended['tau_m']:.2f}")
print(f"tau_u = {recommended['tau_u']:.6f}")

print(f"\nüìä M√âTRICAS DA CONFIGURA√á√ÉO RECOMENDADA:")
print(f"  ‚Ä¢ F1-Score Weighted: {recommended['f1_weighted']:.4f}")
print(f"  ‚Ä¢ F1-Score Macro: {recommended['f1_macro']:.4f}")
print(f"  ‚Ä¢ Balanced Accuracy: {recommended['balanced_accuracy']:.4f}")
print(f"  ‚Ä¢ Accuracy: {recommended['accuracy']:.4f}")
print(f"  ‚Ä¢ Taxa Detec√ß√£o Ataques: {recommended['attack_detection_rate']:.4f}")
print(f"  ‚Ä¢ Recall Benign: {recommended['benign_recall']:.4f}")

print(f"\nüìà DISTRIBUI√á√ÉO DE PREDI√á√ïES:")
print(f"  ‚Ä¢ Benign: {recommended['pred_benign']:,}")
print(f"  ‚Ä¢ (D)DOS: {recommended['pred_ddos']:,}")
print(f"  ‚Ä¢ Botnet: {recommended['pred_botnet']:,}")
print(f"  ‚Ä¢ Brute Force: {recommended['pred_brute_force']:,}")
print(f"  ‚Ä¢ Port Scan: {recommended['pred_port_scan']:,}")
print(f"  ‚Ä¢ Web Attack: {recommended['pred_web_attack']:,}")
print(f"  ‚Ä¢ Unknown: {recommended['pred_unknown']:,}")
print(f"  ‚Ä¢ Total Ataques: {recommended['total_attacks_predicted']:,}")

# Salvar arquivos
csv_path = os.path.join(reports_dir, "threshold_evaluation_complete.csv")
summary_path = os.path.join(reports_dir, "threshold_evaluation_summary.csv")
report_path = os.path.join(reports_dir, "threshold_evaluation_report.txt")

# Salvar dados completos
results_df.to_csv(csv_path, index=False, float_format='%.6f')

# Salvar resumo das melhores configura√ß√µes
main_table.to_csv(summary_path, index=False)

# Salvar relat√≥rio detalhado
with open(report_path, "w") as f:
    f.write("RELAT√ìRIO DE AVALIA√á√ÉO DE THRESHOLDS\n")
    f.write("Multi-Stage Hierarchical Intrusion Detection System\n")
    f.write("=" * 60 + "\n")
    f.write(f"Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Amostras: {len(y_true):,}\n")
    f.write(f"Combina√ß√µes testadas: {len(results):,}\n")
    f.write(f"Tempo execu√ß√£o: {elapsed_time/60:.1f} minutos\n\n")
    
    f.write("DISTRIBUI√á√ÉO REAL DOS DADOS:\n")
    for classe, count in y_true.value_counts().items():
        f.write(f"  {classe}: {count:,} ({count/len(y_true)*100:.1f}%)\n")
    f.write("\n")
    
    f.write("MELHORES CONFIGURA√á√ïES POR M√âTRICA:\n")
    f.write("-" * 60 + "\n")
    for _, config in main_table.iterrows():
        f.write(f"{config['M√©trica']}: {config['Valor']}\n")
        f.write(f"  Thresholds: tau_b={config['tau_b']} tau_m={config['tau_m']} tau_u={config['tau_u']}\n")
        f.write(f"  Predi√ß√µes: {config['Ataques']} ataques, {config['Unknown']} unknown\n")
        f.write(f"  Justificativa: {config['Justificativa']}\n\n")
    
    f.write("CONFIGURA√á√ÉO RECOMENDADA:\n")
    f.write(f"tau_b = {recommended['tau_b']:.6f}\n")
    f.write(f"tau_m = {recommended['tau_m']:.2f}\n")
    f.write(f"tau_u = {recommended['tau_u']:.6f}\n\n")
    f.write("JUSTIFICATIVA:\n")
    f.write("F1-Score Weighted √© a melhor m√©trica para este dataset desbalanceado\n")
    f.write("(95% das amostras s√£o Benign). Ela pondera as classes pelo n√∫mero de amostras,\n")
    f.write("dando mais import√¢ncia √† classifica√ß√£o correta da classe majorit√°ria\n")
    f.write("sem ignorar completamente as classes minorit√°rias.\n")

print(f"\nüíæ ARQUIVOS GERADOS:")
print(f"üìä Dados completos: {csv_path}")
print(f"üìã Resumo das melhores: {summary_path}")
print(f"üìÑ Relat√≥rio detalhado: {report_path}")

print(f"\n‚úÖ AVALIA√á√ÉO CONCLU√çDA!")
print(f"üîÑ Para usar os melhores thresholds, copie os valores recomendados para o codex.py")